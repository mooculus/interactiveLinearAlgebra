\documentclass{ximera}
\input{../preamble.tex}

\title{Gram-Schmidt Orthogonalization} \license{CC BY-NC-SA 4.0}

\begin{document}

\begin{abstract}

\end{abstract}
\maketitle

\begin{onlineOnly}
\section*{Gram-Schmidt Orthogonalization}
\end{onlineOnly}

In \href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RTH-0010/main}{Orthogonality and Projections} we said that a set $\{ \vec{f}_1, \vec{f}_2, \dots, \vec{f}_m\}$ of nonzero vectors in $\RR^n$ is called an \dfn{orthogonal set} if $\vec{f}_i \dotp \vec{f}_j =0$ for all $i \neq j$.  In this section we will prove that every orthogonal set is linearly independent, and therefore it is a basis for its span.  We have already seen that the expansion of a vector as a linear combination of orthogonal basis vectors is easy to obtain because formulas exist for the coefficients.  Hence the orthogonal bases are the ``nice'' bases. %, and much of this chapter is devoted to extending results about bases to orthogonal bases. This leads to some very powerful methods and theorems. 
Our next task is to show that every subspace of $\RR^n$ \textit{has} an orthogonal basis.  We will start with intuitive explorations in lower dimensions, then proceed to formalize our results for subspaces of $\RR^n$.

\subsection*{A Visual Guide to Creating an Orthogonal Basis}  
Given an arbitrary basis $\{\vec{v}_1, \vec{v}_2\}$ of $\RR^2$, let's start building our orthogonal basis, $\{\vec{f}_1, \vec{f}_2\}$, by setting $\vec{f}_1 = \vec{v}_1$. To find the next element of our orthogonal basis, consider the orthogonal projection of $\vec{v}_2$ onto $\vec{f}_1$.  (See the figure below.)  
\begin{center}
\begin{tikzpicture}[scale=1.2]
 \draw[<->] (-1,0)--(5,0);
  \draw[<->] (0,-1)--(0,5);
  \draw[line width=6pt,-stealth, black!20!white](0,0)--(2,4);
\draw[line width=2pt,red,-stealth](0,0)--(1,2);  
\draw[line width=2pt,blue,-stealth](0,0)--(4,3);
\draw[line width=2pt,-stealth](2,4)--(4,3);  
\node[] at (0.7, 2.8)  (p2)    {$\mbox{proj}_{\vec{f}_1}\vec{v}_2$};
\node[] at (3.9, 3.8)  (p2)    {$\vec{f}_2=\vec{v}_2-\mbox{proj}_{\vec{f}_1}\vec{v}_2$};
\node[red] at (-0.1, 1)  (p2)    {$\vec{v}_1=\vec{f}_1$};
\node[blue] at (2, 1.2)  (p2)    {$\vec{v}_2$};
%\draw[line width=2pt,-stealth](0,0)--(2,-1); 
%\node[] at (1.1, -0.3)  (p2)    {$\vec{f}_2$};
 \end{tikzpicture}
 \quad\quad
\begin{tikzpicture}[scale=1.2]
 \draw[<->] (-1,0)--(5,0);
  \draw[<->] (0,-1)--(0,5);
%  \draw[line width=6pt,-stealth, black!20!white](0,0)--(2,4);
\draw[line width=2pt,red,-stealth](0,0)--(1,2);  
%\draw[line width=2pt,blue,-stealth](0,0)--(4,3);
%\draw[line width=2pt,-stealth](2,4)--(4,3);  
%\node[] at (0.7, 2.8)  (p2)    {$\mbox{proj}_{\vec{v}_1}\vec{v}_2$};
%\node[] at (3.9, 3.8)  (p2)    {$\vec{f}_2=\vec{v}_2-\mbox{proj}_{\vec{f}_1}\vec{v}_2$};
\node[red] at (0.2, 1)  (p2)    {$\vec{f}_1$};
%\node[blue] at (2, 1.2)  (p2)    {$\vec{v}_2$};
\draw[line width=2pt,-stealth](0,0)--(2,-1); 
\node[] at (1.1, -0.3)  (p2)    {$\vec{f}_2$};
 \end{tikzpicture}
\end{center}
Next, let $\vec{f}_2=\vec{v}_2-\mbox{proj}_{\vec{f}_1}\vec{v}_2$.  Observe that $\vec{f}_2$ is orthogonal to $\vec{f}_1$ (See Theorem \ref{th:orthDecompX} of \href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RTH-0010/main}{Orthogonality and Projections}).  This gives us an orthogonal collection $\mathcal{B}=\{\vec{f}_1,\vec{f}_2\}$.  It is intuitively clear that $\vec{f}_1$ and $\vec{f}_2$ are linearly independent.  Therefore $\mathcal{B}$ is an orthogonal basis of $\RR^2$.

The following exploration illustrates this process dynamically.

\begin{exploration}\label{exp:orth1}
Choose an arbitrary basis $\{\vec{v}_1, \vec{v}_2\}$ of $\RR^2$ by dragging the tips of vectors $\vec{v}_1$ and $\vec{v}_2$ to desired positions.  Use the navigation bar at the bottom of the interactive window to go through the steps of constructing an orthogonal basis of $\RR^2$.

\pdfOnly{
Access GeoGebra interactives through the online version of this text at 

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro}{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro}.
}

\begin{onlineOnly}
\begin{center}
\geogebra{xtqppyav}{800}{600}
\end{center}
\end{onlineOnly}
\end{exploration}

We can apply this process to any two-dimensional subset of $\RR^n$.  The following exploration will guide you through the process of constructing an orthogonal basis for a plane spanned by two arbitrary vectors in $\RR^3$.

\begin{exploration}\label{exp:orth2}
Let $W =\mbox{span}\left({\bf v}_1,{\bf v}_2\right)$. $W$ is a plane through the origin in $\RR^3$.  Use the navigation bar at the bottom of the interactive window to go through the steps of constructing an orthogonal basis for $W$.  RIGHT-CLICK and DRAG to rotate the image for a better view.

\pdfOnly{
Access GeoGebra interactives through the online version of this text at 

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro}{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro}.
}

\begin{onlineOnly}
    \begin{center}
\geogebra{zghsfkym}{900}{600}
\end{center}
\end{onlineOnly}
\end{exploration}

In the next exploration, we take the process of constructing an orthogonal basis to the edge of the visual realm and construct an orthogonal basis for $\RR^3$.

\begin{exploration}\label{exp:orth3}
In the GeoGebra interactive below $\{\vec{v}_1, \vec{v}_2, \vec{v}_3\}$ is a basis of $\RR^3$.  Use check boxes to go through the steps for constructing an orthogonal basis starting with the given basis.  RIGHT-CLICK and DRAG to rotate the image for a better view.

\pdfOnly{
Access GeoGebra interactives through the online version of this text at 

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro}{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro}.
}

\begin{onlineOnly}
\begin{center}
\geogebra{qjpvmsws}{900}{800}
\end{center}
\end{onlineOnly}
\end{exploration}


\subsection*{Gram-Schmidt Orthogonalization Algorithm}

In \href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RTH-0010/main}{Orthogonality and Projections} we have repeatedly assumed that our subspaces of $\RR^n$ have an orthogonal basis.  We will now prove that this is indeed the case.  Recall that to be a basis of a subspace, a set of vectors must be linearly independent and it must span the subspace.  We will start by demonstrating that a set of orthogonal vectors must be linearly independent.

\begin{theorem}\label{orthbasis}
Let $ \{ \vec{w}_1, \vec{w}_2, \cdots, \vec{w}_k \}$ be an
orthogonal set of non-zero vectors in $\RR^n$. Then this set is
linearly independent.% and forms a basis for the subspace $W =
%\mbox{span}\left( \vec{w}_1, \vec{w}_2, \cdots, \vec{w}_k \right)$.
\end{theorem}

\begin{proof}
To show that this set is linearly independent, we need to demonstrate that the only solution to the following equation is the trivial solution.
\[
a_1 \vec{w}_1 + a_2 \vec{w}_2 + \cdots + a_k \vec{w}_k = \vec{0}
\]
To accomplish this, we need to show that all $a_i = 0$ for all $0\leq i\leq k$.  To do so we take the dot product of
each side of the above equation with the vector $\vec{w}_i$ and obtain the following.

\begin{eqnarray*}
\vec{w}_i \dotp (a_1 \vec{w}_1 + a_2 \vec{w}_2 + \cdots + a_k \vec{w}_k ) &=& \vec{w}_i \dotp \vec{0}\\
a_1 (\vec{w}_i \dotp \vec{w}_1) + a_2 (\vec{w}_i \dotp \vec{w}_2) + \cdots + a_k (\vec{w}_i \dotp \vec{w}_k)  &=& 0
\end{eqnarray*}
Now since the set is orthogonal, $\vec{w}_i \dotp \vec{w}_m = 0$ for all $m \neq i$, so we have:
\[
a_1 (0) + \cdots + a_i(\vec{w}_i \dotp \vec{w}_i) + \cdots + a_k (0) = 0
\]
\[
a_i \norm{\vec{w}_i}^2 = 0
\]

We know that $\norm{\vec{w}_i}^2  \neq 0$, so it follows that $a_i =0$. Since $i$ was chosen arbitrarily, $a_i =0$ for all $i$ $(0\leq i\leq k)$. This proves that $\{ \vec{w}_1, \vec{w}_2, \cdots, \vec{w}_k \}$ is linearly independent.

%Finally since $W = \mbox{span} \{ \vec{w}_1, \vec{w}_2, \cdots,
%\vec{w}_k \}$, the set of vectors also spans $W$ and therefore forms a basis of $W$.

\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The following theorem shows how to start with an arbitrary basis of a subspace $W$ of $\RR^n$ and find an orthogonal basis for $W$.  To better understand the notation and the process presented in this theorem, you may want to match the steps of the theorem to the steps of Exploration \ref{exp:orth3}.

\begin{theorem}[Gram-Schmidt Orthogonalization] \label{th:GS}
If $\{\vec{v}_{1}, \vec{v}_{2}, \dots , \vec{v}_{m}\}$ is any basis of a subspace $W$ of $\RR^n$, consider the following sequence of subspaces:
\begin{equation*}
\begin{array}{ccl}
W_1&=&\mbox{span}\{\vec{v}_{1}\} \\
W_2&=&\mbox{span}\{\vec{v}_{1},\vec{v}_{2}\} \\
W_3&=&\mbox{span}\{\vec{v}_{1},\vec{v}_{2},\vec{v}_{3}\} \\
\vdots &&\\
W_m &=& \mbox{span}\{\vec{v}_{1},\vec{v}_{2},\vec{v}_{3},\ldots,\vec{v}_{m}\}
\end{array}
\end{equation*}

Then we can construct an orthogonal basis $\{\vec{f}_{1},\vec{f}_{2},\ldots,\vec{f}_{m}\}$ for $W_k$ for each $k = 2, 3, \dots , m$ by adding one vector at a time successively as follows:
\begin{equation*}
\begin{array}{ccl}
\vec{f}_{1} &=& \vec{v}_{1} \\
\vec{f}_{2} &=& \vec{v}_{2} - \mbox{proj}_{W_1}(\vec{v}_2) \\
\vec{f}_{3} &=& \vec{v}_{3} - \mbox{proj}_{W_2}(\vec{v}_3) \\
\vdots &&\\
\vec{f}_{m} &=& \vec{v}_{m} - \mbox{proj}_{W_{m-1}}(\vec{v}_m)
\end{array}
\end{equation*}

Then, $\{\vec{f}_{1},\vec{f}_{2},\ldots,\vec{f}_{m}\}$ will be an orthogonal basis for $W$.  
\end{theorem}
\begin{proof}
Using the definition of projection onto a subspace, the iterative procedure above may be written:
\begin{equation}\label{eqn:GSproof}
\begin{array}{ccl}
\vec{f}_{1} &=& \vec{v}_{1} \\
\vec{f}_{2} &=& \vec{v}_{2} - \frac{\vec{v}_{2} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} \\
\vec{f}_{3} &=& \vec{v}_{3} - \frac{\vec{v}_{3} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} - \frac{\vec{v}_{3} \dotp \vec{f}_{2}}{\norm{\vec{f}_{2}}^2}\vec{f}_{2} \\
\vdots &&\\
\vec{f}_{k} &=& \vec{v}_{k} - \frac{\vec{v}_{k} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} - \frac{\vec{v}_{k} \dotp \vec{f}_{2}}{\norm{\vec{f}_{2}}^2}\vec{f}_{2} - \dots -\frac{\vec{v}_{k} \dotp \vec{f}_{k-1}}{\norm{\vec{f}_{k-1}}^2}\vec{f}_{k-1} \\
\vdots &&
\end{array}
\end{equation}
We see immediately that $\mbox{span}\{\vec{f}_{1}\}=W_1$ and that $\mbox{span}\{\vec{f}_{1},\vec{f}_{2}\}=W_2$ because $\vec{f}_{2}$ is a linear combination of $\vec{v}_{1}$ and $\vec{v}_{2}$.  In fact, for any value of $k$, we see that $\mbox{span}\{\vec{f}_{1},\vec{f}_{2},\ldots,\vec{f}_{k}\}=W_k$, because each $\vec{f}_{k}$ is a linear combination of the vectors $\{\vec{v}_{1},\vec{v}_{2},\ldots,\vec{v}_{k-1}\}$.

Repeated application of Corollary \ref{cor:orthProjOntoW} shows that the set
 $\{\vec{f}_{1},\vec{f}_{2},\ldots,\vec{f}_{m}\}$ is orthogonal.  Linear independence follows from orthogonality by Theorem \ref{orthbasis}.  

We conclude that $\{\vec{f}_{1},\vec{f}_{2},\ldots,\vec{f}_{m}\}$ is a linearly independent orthogonal set that spans $W$.
\end{proof}


\begin{remark}\label{rem:SchmidtPederson}Erhardt
 Schmidt (1876--1959) was a German mathematician who studied under the
great David Hilbert. He
 first described the present algorithm in 1907. J\"{o}rgen Pederson Gram
(1850--1916)\index{Gram, J\"{o}rgen Pederson}  was a Danish actuary.
\end{remark}

\begin{example}\label{exa:023743}
Find an orthogonal basis of the row space of $A = \begin{bmatrix}
1 & 1 & -1 & -1\\
3 & 2 & 0 & 1\\
1 & 0 & 1 & 0
\end{bmatrix}$.

\begin{explanation}
  Let $\vec{v}_{1}$, $\vec{v}_{2}$, $\vec{v}_{3}$ denote the rows of $A$ and observe that $\{\vec{v}_{1}, \vec{v}_{2}, \vec{v}_{3}\}$ is linearly independent. Take $\vec{f}_{1} = \vec{v}_{1}$. The algorithm gives
\begin{eqnarray*}
\vec{f}_{2} &=& \vec{v}_{2} - \frac{\vec{v}_{2} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} = [3, 2, 0, 1] - \frac{4}{4}[1, 1, -1, -1] = [2, 1, 1, 2] \\
\vec{f}_{3} &=& \vec{v}_{3} - \frac{\vec{v}_{3} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} - \frac{\vec{v}_{3} \dotp \vec{f}_{2}}{\norm{\vec{f}_{2}}^2}\vec{f}_{2} = \vec{v}_{3} - \frac{0}{4}\vec{f}_{1} - \frac{3}{10}\vec{f}_{2} = \frac{1}{10}[4, -3, 7, -6]
\end{eqnarray*}

Hence $\{[1, 1, -1, -1], [2, 1, 1, 2], \frac{1}{10}[4, -3, 7, -6]\}$ is the orthogonal basis provided by the algorithm. In
hand calculations it may be convenient to eliminate fractions (see the Remark below), so $\{[1, 1, -1, -1], [2, 1, 1, 2], [4, -3, 7, -6]\}$ is also an orthogonal basis for $\mbox{row}(A)$.
\end{explanation}
\end{example}

\begin{remark}\label{rem:scalarMultGS}
Observe that the vector $\frac{\vec{x} \dotp \vec{f}_{i}}{\norm{\vec{f}_{i}}^2}\vec{f}_{i}$
 is unchanged if a nonzero scalar multiple of $\vec{f}_{i}$ is used in place of $\vec{f}_{i}$. Hence, if a newly constructed $\vec{f}_{i}$ is multiplied by a nonzero scalar at some stage of the Gram-Schmidt algorithm, the subsequent $\vec{f}$s will be unchanged. This is useful in actual calculations.
 \end{remark}

The Gram-Schmidt algorithm demonstrates in a constructive way that every subspace of $\RR^n$ has an orthogonal basis.  We formalize this in one final theorem.

\begin{theorem}\label{023635}
Let $W$ be a subspace of $\RR^n$.  Then  $W$ has an orthogonal basis.  In fact, every orthogonal subset $\{\vec{f}_{1}, \dots , \vec{f}_{m}\}$ in $W$ can be extended to an orthogonal basis for $W$.
\end{theorem}

\begin{proof}
Suppose $\{\vec{f}_{1}, \dots , \vec{f}_{m}\}$ is an orthogonal subset of $W$.  If $\mbox{span}\left(\vec{f}_{1}, \dots , \vec{f}_{m}\right) = W$, it is already a basis. Otherwise, there exists $\vec{x}$ in $W$ outside $\mbox{span}\left(\vec{f}_{1}, \dots , \vec{f}_{m}\right)$. Using the Gram-Schmidt procedure we define $\vec{f}_{m+1} = \vec{x} - \mbox{proj}_{W_{m}}(\vec{x})$, where $W_m = \mbox{span}\{\vec{f}_{1},\vec{f}_{2},\ldots,\vec{f}_{m}\}$. If $\mbox{span}\left(\vec{f}_{1}, \dots, \vec{f}_{m}, \vec{f}_{m+1}\right) = W$, we are done. Otherwise, the process continues to create larger and larger orthogonal subsets of $W$. They are all linearly independent by Theorem~\ref{th:GS}, so we have a basis when we reach a subset containing $\mbox{dim}(W)$ vectors.
\end{proof}

The process described in the proof of this theorem is used in this final example.

\begin{example}\label{ex:GSextend}
In Example~\ref{exa:023743}, given $A = \begin{bmatrix}
1 & 1 & -1 & -1\\
3 & 2 & 0 & 1\\
1 & 0 & 1 & 0
\end{bmatrix}$, we showed that an orthogonal basis for $\mbox{row}(A)$ is given by $$\{\vec{f}_1=[1, 1, -1, -1], \vec{f}_2=[2, 1, 1, 2], \vec{f}_3=[4, -3, 7, -6]\}.$$

Choose any vector $\vec{v}_4 \in \RR^4$ not in $\mbox{span}\{\vec{f}_1, \vec{f}_2, \vec{f}_3\}$, and apply the Gram-Schmidt algorithm to produce a vector $\vec{f}_4$ such that $\{\vec{f}_1, \vec{f}_2, \vec{f}_3, \vec{f}_4\}$ is an orthogonal basis for $\RR^4$.


\begin{explanation}
  Let $\vec{v}_4 = [1, 0, 0, 0]$.  (How would you check that $\vec{v}_4$ is not in $\mbox{span}\{\vec{f}_1, \vec{f}_2, \vec{f}_3\}$?)  To get a vector $\vec{f}$ orthogonal to the row space, we perform an iteration of Gram-Schmidt:
  \begin{eqnarray*}
      \vec{f} &=& \vec{v}_{4} - \frac{\vec{v}_{4} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} - \frac{\vec{v}_{4} \dotp \vec{f}_{2}}{\norm{\vec{f}_{2}}^2}\vec{f}_{2} - \frac{\vec{v}_{4} \dotp \vec{f}_{3}}{\norm{\vec{f}_{3}}^2}\vec{f}_{3} \\
      &=& [1, 0, 0, 0] - \frac{1}{4}[1, 1, -1, -1] - \frac{2}{10}[2, 1, 1, 2] - \frac{4}{110}[4, -3, 7, -6] \\
      &=&\frac{1}{44}[9, -15, -9, 3]
  \end{eqnarray*}
  Since any multiple of $\vec{f}$ will suffice, we are free to choose $\vec{f}_{4} = 44\vec{f} = [9, -15, -9, 3]$ to get rid of the fraction.  It is easy to check that $\{\vec{f}_1, \vec{f}_2, \vec{f}_3, \vec{f}_4\}$ is an orthogonal set, and it follows from Theorem~\ref{orthbasis} that this set is a basis for $\RR^4$.
\end{explanation}
\end{example}

\begin{remark}\label{rem:vectorInSpan}
Suppose instead of $[1,0,0,0]$ we had started with $\vec{v}_4 = [7, -1, 7, -5]$.  This vector $\vec{v}_4$ is in $\mbox{span}\{\vec{f}_1, \vec{f}_2, \vec{f}_3\}$, as it is the sum of those three vectors.  But if we were to try to proceed as above, we would get 
  \begin{eqnarray*}
      \vec{f} &=& \vec{v}_{4} - \frac{\vec{v}_{4} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} - \frac{\vec{v}_{4} \dotp \vec{f}_{2}}{\norm{\vec{f}_{2}}^2}\vec{f}_{2} - \frac{\vec{v}_{4} \dotp \vec{f}_{3}}{\norm{\vec{f}_{3}}^2}\vec{f}_{3} \\
      &=& [7, -1, 7, -5] - \frac{4}{4}[1, 1, -1, -1] - \frac{10}{10}[2, 1, 1, 2] - \frac{110}{110}[4, -3, 7, -6] \\
      &=&[0,0,0,0]
  \end{eqnarray*}
We could not add a multiple of $\vec{f}$ to $\{\vec{f}_1, \vec{f}_2, \vec{f}_3\}$ to get an orthogonal basis for $\RR^4$.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Practice Problems}

\begin{problem}\label{prob:extend}

    Try Example \ref{ex:GSextend} again starting with some other vector $\vec{v}_4 \in \RR^4$.

\end{problem}

\emph{Problems \ref{GS1}-\ref{GS4}}

In each case, use the Gram-Schmidt algorithm to convert the given basis $\mathcal{B}$ of $V$ to an orthogonal basis.  

\begin{problem}\label{GS1}
$V = \RR^2$, $\mathcal{B} = \left\{\begin{bmatrix}1\\ -1\end{bmatrix}, \begin{bmatrix}2\\ 1\end{bmatrix}\right\}$

% Click on the arrow to see one possible answer:
% \begin{expandable}{}{}
% $\left\{\begin{bmatrix}2\\1\end{bmatrix},\begin{bmatrix}-3/5\\6/5\end{bmatrix}\right\}$
% \end{expandable}
\end{problem}

\begin{problem}\label{GS2}
$V = \RR^2$, $\mathcal{B} = \left\{\begin{bmatrix}2\\ 1\end{bmatrix}, \begin{bmatrix}1\\ 2\end{bmatrix}\right\}$
\end{problem}

\begin{problem}\label{GS3}
$V = \RR^3$, $\mathcal{B} = \left\{\begin{bmatrix}1\\ -1\\ 1\end{bmatrix}, \begin{bmatrix}1\\ 0\\ 1\end{bmatrix}, \begin{bmatrix}1\\ 1\\ 2\end{bmatrix}\right\}$
\end{problem}

\begin{problem}\label{GS4}
$V = \RR^3$, $\mathcal{B} = \left\{\begin{bmatrix}0\\ 1\\ 1\end{bmatrix}, \begin{bmatrix}1\\ 1\\ 1\end{bmatrix}, \begin{bmatrix}1\\ -2\\ 2\end{bmatrix}\right\}$

% Click the arrow to see one possible answer:
% \begin{expandable}{}{}
% $\left\{\begin{bmatrix}0\\1\\1\end{bmatrix},\begin{bmatrix}1\\1\\1\end{bmatrix},\begin{bmatrix}1\\-2\\2\end{bmatrix}\right\}$
% \end{expandable}
\end{problem}


% \begin{problem}
% Let $A$ be an $n \times n$ matrix of rank $r$. Show that there is an invertible $n \times n$ matrix $M$ such that $MA$ is a row-echelon matrix with the property that the first $r$ rows are orthogonal.
% \begin{hint}
% Let $R$ be the row-echelon form of $A$, and use the Gram-Schmidt process on the nonzero rows of $R$ from the bottom up.
% \end{hint}
% \end{problem}



\section*{Text Source} This section was adapted from the first part of Section 8.1 of Keith Nicholson's \href{https://open.umn.edu/opentextbooks/textbooks/linear-algebra-with-applications}{\it Linear Algebra with Applications}. (CC-BY-NC-SA)

W. Keith Nicholson, {\it Linear Algebra with Applications}, Lyryx 2018, Open Edition, p. 415 


\end{document}