\documentclass{ximera}
\input{../preamble.tex}

\title{INDEX} \license{CC BY-NC-SA 4.0}



\begin{document}
\begin{abstract}
\end{abstract}
\maketitle


\begin{onlineOnly}
\section*{INDEX}
\end{onlineOnly}

A hyperlinked term will take you to the section where the term is defined.  Parenthetical hyperlink will take you to the specific definition or formula in the section.  Use arrows on the right to display the definition or formula in the index.
\subsection*{A}
\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VEC-0030/main}{addition of vectors}

%adjoint of a matrix

adjugate of a matrix (the term \emph{adjoint} is also sometimes used) (\ref{th:adjugateinverseformula})
\begin{expandable}{}{}
    The transpose of the matrix of cofactors of a matrix - it is part of a formula for the inverse of a matrix.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/EIG-0020/main}{algebraic multiplicity of an eigenvalue}
\begin{expandable}{}{}
    The multiplicity of an eigenvalue as a root of the characteristic equation.
\end{expandable}

associated homogeneous system (\ref{def:asshomsys})
\begin{expandable}{}{}
    Given any linear system $A\vec{x}=\vec{b}$, the system $A\vec{x}=\vec{0}$ is called the \dfn{associated homogeneous system}.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0020/main}{augmented matrix}
\begin{expandable}{}{}
    Every linear system 
$$\begin{array}{ccccccccc}
      a_{11}x_1 &+ &a_{12}x_2&+&\ldots&+&a_{1n}x_n&= &b_1 \\
	 a_{21}x_1 &+ &a_{22}x_2&+&\ldots&+&a_{2n}x_n&= &b_2 \\
     &&&&\vdots&&&& \\
     a_{m1}x_1 &+ &a_{m2}x_2&+&\ldots&+&a_{mn}x_n&= &b_m
    \end{array}$$
    can be written in the \dfn{augmented matrix form} as follows:
    $$\left[\begin{array}{cccc|c}  
 a_{11}&a_{12}&\ldots&a_{1n}&b_1\\a_{21}&a_{22}&\ldots&a_{2n}&b_2\\\vdots&\vdots&\ddots&\vdots&\vdots\\a_{m1}&a_{m2}&\ldots&a_{mn}&b_m
 \end{array}\right]$$
 The array to the left of the vertical bar is called the \dfn{coefficient matrix} of the linear system and is often given a capital letter name, like $A$.  The vertical array to the right of the bar is called a \dfn{constant vector}.
 $$A=\begin{bmatrix}a_{11}&a_{12}&\ldots&a_{1n}\\a_{21}&a_{22}&\ldots&a_{2n}\\\vdots&\vdots&\ddots&\vdots\\a_{m1}&a_{m2}&\ldots&a_{mn}\end{bmatrix}\quad\text{and}\quad\vec{b}=\begin{bmatrix}b_1\\b_2\\\vdots\\b_m\end{bmatrix}$$
We will sometimes use the following notation to represent an augmented matrix.

$$\left[\begin{array}{c|c}  
 A & \vec{b}\\
 \end{array}\right]$$
\end{expandable}

\subsection*{B}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0020/main}{back substitution}
\begin{expandable}{}{}
When a matrix is in row-echelon form, we can compute the solution to the system by starting from the last equation and working backwards.  This process is known as \dfn{back substitution}.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0020/main}{basic variable} (also called a \dfn{leading variable}
\begin{expandable}{}{}
    When a coefficient matrix is in row echelon form, a \dfn{basic variable} is a variable corresponding to a column of the matrix with at least one leading entry.
\end{expandable}

basis (\ref{def:basis})
\begin{expandable}{}{}
    A set $\mathcal{S}$ of vectors is called a \dfn{basis} of $\RR^n$ (or a basis of a subspace $V$ of $\RR^n$) provided that 
\begin{enumerate}
\item 
$\mbox{span}(\mathcal{S})=\RR^n$ (or $V$)
\item 
$\mathcal{S}$ is linearly independent.
\end{enumerate}
\end{expandable}

%Basis Theorem

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0023/main}{block matrices}
\begin{expandable}{}{}
    Subdividing a matrix into submatrices using imaginary horizontal and vertical lines - used to multiply matrices more efficiently.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/DET-0070/main}{box product}

\subsection*{C}

change-of-basis matrix (\ref{def:matlintransgenera})
\begin{expandable}{}{}
    Matrix $A$ of Theorem \ref{th:matlintransgeneral} is called the matrix of $T$ with respect to ordered bases $\mathcal{B}$ and $\mathcal{C}$.
\end{expandable}

characteristic equation (\ref{def:chareqcharpoly})
\begin{expandable}{}{}
    The equation 
$$\mbox{det}(A-\lambda I) = 0$$ is called the \dfn{characteristic equation} of $A$. 
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/EIG-0020/main}{characteristic polynomial}
\begin{expandable}{}{}
    The polynomial 
$$\mbox{det}(A-\lambda I)$$ is called the \dfn{characteristic equation} of $A$. 
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RTH-0045/main}{Cholesky factorization}

closed under addition (\ref{def:closedunderaddition})
\begin{expandable}{}{}
    A set $V$ is said to be \dfn{closed under addition} if for each element $\vec{u} \in V$ and $\vec{v} \in V$ the sum $\vec{u}+\vec{v}$ is also in $V$.
\end{expandable}

closed under scalar multiplication (\ref{def:closedunderscalarmult})
\begin{expandable}{}{}
    A set $V$ is said to be \dfn{closed under scalar multiplication} if for each element $\vec{v} \in V$  and for each scalar $k \in \RR$ the product $k\vec{v}$ is also in $V$.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/LTR-0010/main}{codomain of a linear transformation}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0020/main}{coefficient matrix}
\begin{expandable}{}{}
    A \dfn{coefficient matrix} is a matrix whose entries are the coefficients of a system of linear equations.  For the system $$\begin{array}{ccccccccc}
      a_{11}x_1 &+ &a_{12}x_2&+&\ldots&+&a_{1n}x_n&= &b_1 \\
	 a_{21}x_1 &+ &a_{22}x_2&+&\ldots&+&a_{2n}x_n&= &b_2 \\
     &&&&\vdots&&&& \\
     a_{m1}x_1 &+ &a_{m2}x_2&+&\ldots&+&a_{mn}x_n&= &b_m
    \end{array},$$ 
    the \dfn{coefficient matrix} is $A=\begin{bmatrix}a_{11}&a_{12}&\ldots&a_{1n}\\a_{21}&a_{22}&\ldots&a_{2n}\\\vdots&\vdots&\ddots&\vdots\\a_{m1}&a_{m2}&\ldots&a_{mn}\end{bmatrix}$.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/DET-0010/main}{cofactor expansion}
\begin{expandable}{}{}
    A method to compute $\mbox{det} A$ using determinants of minor matrices associated with one row or one column.
\end{expandable}


\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0010/main}{column matrix (vector)}
\begin{expandable}{}{}
    A matrix with $m$ rows and only 1 column.
\end{expandable}

column space of a matrix (\ref{def:colspace})
\begin{expandable}{}{}
    Let $A$ be an $m\times n$ matrix.  The \dfn{column space} of $A$, denoted by $\mbox{col}(A)$, is the subspace of $\RR^m$ spanned by the columns of $A$.
\end{expandable}

composition of linear transformations (\ref{def:compoflintrans})
\begin{expandable}{}{}
    Let $U$, $V$ and $W$ be vector spaces, and let $T:U\rightarrow V$ and $S:V\rightarrow W$ be linear transformations.  The \dfn{composition} of $S$ and $T$ is the transformation $S\circ T:U\rightarrow W$ given by
$$(S\circ T)(\vec{u})=S(T(\vec{u}))$$

The matrix of a composition is the product of the matrices corresponding to the transformations in the composition, in the same order.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0010/main}{consistent system}
\begin{expandable}{}{}
    A system of equations that has at least one solution.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0040/main}{convergence}
\begin{expandable}{}{}
    when the iterates of an iterative method approach a solution
\end{expandable}

coordinate vector with respect to a basis (\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VSP-0030/main}{in $\RR^n$}) (\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VSP-0060/main}{abstract vector spaces})
\begin{expandable}{}{}
    Let $\mathcal{B} = \{\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_k\}$ be an \underline{ordered} basis.  Then the \dfn{coordinate vector} $\vec{v}$ is the  column vector $\begin{bmatrix}c_1\\ c_2\\ \vdots \\c_k\end{bmatrix}$ such that $\vec{v} = c_1\vec{v}_1+c_2\vec{v}_2+\ldots +c_p\vec{v}_k$.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/DET-0060/main}{Cramer's Rule}
\begin{expandable}{}{}
    A method of solving systems of equations that uses determinants.
\end{expandable}

cross product (\ref{def:crossproduct})
\begin{expandable}{}{}
    Let $\vec{u=\begin{bmatrix}u_1\\u_2\\u_3\end{bmatrix}}$ and $\vec{v}=\begin{bmatrix}v_1\\v_2\\v_3\end{bmatrix}$ be vectors in $\RR^3$.  The \dfn{cross product} of $\vec{u}$ and $\vec{v}$, denoted by $\vec{u}\times\vec{v}$, is given by
$$\vec{u}\times\vec{v}=(u_2v_3-u_3v_2)\vec{i}-(u_1v_3-u_3v_1)\vec{j}+(u_1v_2-u_2v_1)\vec{k}
=\begin{bmatrix}u_2v_3-u_3v_2\\-u_1v_3+u_3v_1\\u_1v_2-u_2v_1\end{bmatrix}$$
\end{expandable}

\subsection*{D}
\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0025/main}{determinant}
\begin{expandable}{}{}
    A function that assigns a scalar output to each square matrix $A$, denoted $\mbox{det} A$ - it is nonzero if and only if $A$ is invertible.  Geometrically speaking, the determinant of a linear transformation of a square matrix is the factor by which area (or volume or hypervolume) is scaled by the transformation.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0025/main}{diagonal matrix}
\begin{expandable}{}{}
    A matrix $A=[a_{ij}]$  where $a_{ij}=0$ whenever $i \ne j$
\end{expandable}

diagonalizable matrix (\ref{def:diagonalizable})
\begin{expandable}{}{}
    Let $A$ be an $n\times n$ matrix. Then $A$ is said to be \dfn{diagonalizable} if there exists an invertible matrix $P$ such that
\begin{equation*}
P^{-1}AP=D
\end{equation*}
where $D$ is a diagonal matrix.  In other words, a matrix $A$ is diagonalizable if it is similar to a diagonal matrix, $A \sim D$.
\end{expandable}

dimension (\ref{def:dimension}) (also see \ref{def:dimensionabstract})
\begin{expandable}{}{}
    Let $V$ be a subspace of $\RR^n$.  The \dfn{dimension} of $V$ is the number, $m$, of elements in any basis of $V$.  We write
$$\mbox{dim}(V)=m$$
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RRN-0010/main}{Dimensions of a matrix}
\begin{expandable}{}{}
    An $m \times n$ matrix is a matrix with $m$ rows and $n$ columns.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RRN-0020/main}{Direction vector}

Distance between points in $\RR^n$ (\ref{form:distRn})
\begin{expandable}{}{}
%\begin{formula}
Let $A(a_1, a_2,\ldots ,a_n)$ and $B(b_1, b_2,\ldots ,b_n)$ be points in $\RR^n$.  The distance between $A$ and $B$ is given by
$$AB=\sqrt{(a_1-b_1)^2+(a_2-b_2)^2+\ldots +(a_n-b_n)^2}$$
%\end{formula}
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VEC-0070/main}{Distance between point and line}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0040/main}{divergence}
\begin{expandable}{}{}
    when the iterates of an iterative method fail to approach a solution
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/LTR-0010/main}{domain of a linear transformation} 

dominant eigenvalue (\ref{def:dominant ew,ev})
\begin{expandable}{}{}
    An eigenvalue $\lambda$ of an $n \times n$ matrix $A$ is called a \dfn{dominant eigenvalue} if $\lambda$ has multiplicity $1$, and
\begin{equation*}
|\lambda| > |\mu| \quad \mbox{ for all eigenvalues } \mu \neq \lambda
\end{equation*}
Any corresponding eigenvector is called a \dfn{dominant eigenvector} of $A$.
\end{expandable}

dot product (\ref{def:dotproduct})
\begin{expandable}{}{}
Let $\vec{u}$ and $\vec{v}$ be vectors in $\RR^n$.  The \dfn{dot product} of $\vec{u}$ and $\vec{v}$, denoted by $\vec{u}\dotp \vec{v}$, is given by
$$\vec{u}\dotp\vec{v}=\begin{bmatrix}u_1\\u_2\\\vdots\\u_n\end{bmatrix}\dotp\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}=u_1v_1+u_2v_2+\ldots+u_nv_n$$
\end{expandable}


\subsection*{E}

eigenspace (\ref{def:eigspace})
\begin{expandable}{}{}
    If $\lambda$ is an eigenvalue of an $n \times n$ matrix, the set of all eigenvectors associated to $\lambda$ along with the zero vector is the \dfn{eigenspace} associated to $\lambda$.  The eigenspace is a subspace of $\RR^n$.
\end{expandable}

eigenvalue (\ref{def:eigen})
\begin{expandable}{}{}
    Let $A$ be an $n \times n$ matrix.  We say that a scalar $\lambda$ is an \dfn{eigenvalue} of $A$ if $$A\vec{x} = \lambda \vec{x}$$
for some nonzero vector $x$.
We say that $x$ is an \dfn{eigenvector} of $A$ associated with the eigenvalue $\lambda$.
\end{expandable}

eigenvalue decomposition (\ref{def:eigdecomposition})
\begin{expandable}{}{}
    If we are able to diagonalize $A$, say $A=PDP^{-1}$, we say that $PDP^{-1}$ is an \dfn{eigenvalue decomposition} of $A$.
\end{expandable}

eigenvector (\ref{def:eigen})
\begin{expandable}{}{}
    Let $A$ be an $n \times n$ matrix.  We say that a non-zero vector $\vec{x}$ is an \dfn{eigenvector} of $A$ if $$A\vec{x} = \lambda \vec{x}$$
for some scalar $\lambda$.
We say that $\lambda$ is an \dfn{eigenvalue} of $A$ associated with the eigenvector $\vec{x}$.
\end{expandable}

elementary matrix (\ref{def:elemmatrix})
\begin{expandable}{}{}
    An \dfn{elementary matrix} is a square matrix formed by applying a single elementary row operation to the identity matrix.
\end{expandable}

elementary row operations (\ref{def:elemrowops})
\begin{expandable}{}{}
    The following three operations performed on a linear system are called \dfn{elementary row operations}.
\begin{enumerate}
\item Switching the order of equations (rows) $i$ and $j$:
$$R_i\leftrightarrow R_j$$
\item Multiplying both sides of equation (row) $i$ by the same non-zero constant, $k$, and replacing equation $i$ with the result:
$$kR_i\rightarrow R_i$$
\item Adding $k$ times equation (row) $i$ to equation (row) $j$, and replacing equation $j$ with the result:
$$R_j+kR_i\rightarrow R_j$$
\end{enumerate}
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/EIG-0040/main}{equivalence relation}

equivalent linear systems (\ref{def:equivsystems})
\begin{expandable}{}{}
    Linear systems are called \dfn{equivalent} if they have the same solution set.
\end{expandable}

%Euclidean norm


\subsection*{F}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0020/main}{free variable}
\begin{expandable}{}{}
    When a linear system is in row-echelon form, the variables corresponding to columns that do not have any leading coefficients (if there are any) are known as \dfn{free variables}.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VSP-0040/main}{fundamental subspaces of a matrix}
(See also \href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RTH-0020/main}{Orthogonal Complements and Decompositions}.)
\begin{expandable}{}{}
    $\mbox{null}(A)$ is the orthogonal complement of $\mbox{row}(A)$, and $\mbox{null}(A^T)$ is the orthogonal complement of $\mbox{col}(A)$
\end{expandable}

\subsection*{G}

Gauss-Jordan elimination (\ref{def:GaussJordanElimination})
\begin{expandable}{}{}
    The process of using the elementary row operations on a matrix to transform it into reduced row-echelon form is called \dfn{Gauss-Jordan elimination}.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0040/main}{Gauss-Seidel method}
\begin{expandable}{}{}
    An iterative method for solving linear systems that is a refinement of the Jacobi method, where we use computed values of variables alternately for quicker convergence.
\end{expandable}

Gaussian elimination (\ref{def:GaussianElimination})
\begin{expandable}{}{}
    The process of using the elementary row operations on a matrix to transform it into row-echelon form is called \dfn{Gaussian Elimination}.
\end{expandable}

geometric multiplicity of an eigenvalue (\ref{def:geommulteig})
\begin{expandable}{}{}
    The \dfn{geometric multiplicity} of an eigenvalue $\lambda$ is the dimension of the corresponding eigenspace $\mathcal{S}_\lambda$.
\end{expandable}

Gershgorin disk
(\ref{th:Gershgorin})
\begin{expandable}{}{}
    A circle in the complex plane which has a diagonal entry of a matrix as its center and the sum of the absolute values of the other entries in that row (or column) as its radius.
\end{expandable}

Gershgorin's Theorem
(\ref{th:Gershgorin})
\begin{expandable}{}{}
    \dfn{Gershgorin’s theorem} says that the $n$ eigenvalues of an $n \times n$ matrix can be found in the region in the complex plane consisting of the $n$ Gershgorin disks.
\end{expandable}

Gram-Schmidt process (\ref{th:GS})
\begin{expandable}{}{}
    An iterative process which constructs an orthogonal basis for a subspace. The idea is to build the orthogonal set one vector at a time, by taking a vector not in the span of the vectors in the current iteration of the set, and subtracting its orthogonal projection onto each of those vectors.
\end{expandable}

\subsection*{H}
% Head - Tail Formula (\ref{form:headminustailrn})
% \begin{expandable}{}{}
% %    \begin{formula}
%   [``Head - Tail'' Formula in $\RR^n$]
% Suppose a vector's tail is at point $A(a_1, a_2, \ldots ,a_n)$ and the vector's head is at $B(b_1, b_2, \ldots ,b_n)$, then 
% $$\overrightarrow{AB}=\begin{bmatrix}b_1-a_1\\ b_2-a_2\\ \vdots \\b_n-a_n\end{bmatrix}$$

% %\end{formula}
% \end{expandable}

homogeneous system (\ref{def:homogeneous})
\begin{expandable}{}{}
    A system of linear equations is called \dfn{homogeneous} if the system can be written in the form
$$\begin{array}{ccccccccc}
      a_{11}x_1 &+ &a_{12}x_2&+&\ldots&+&a_{1n}x_n&= &0 \\
	 a_{21}x_1 &+ &a_{22}x_2&+&\ldots&+&a_{2n}x_n&= &0 \\
     &&&&\vdots&&&& \\
     a_{m1}x_1 &+ &a_{m2}x_2&+&\ldots&+&a_{mn}x_n&= &0
    \end{array},$$
or as a matrix equation as $A \vec{x} = \vec{0}$.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RRN-0030/main}{hyperplane}

\subsection*{I}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0020/main}{identity matrix}
\begin{expandable}{}{}
    A square matrix with ones as diagonal entries and zeros for the remaining entries.
\end{expandable}

identity transformation (\ref{def:idtransonrn})
\begin{expandable}{}{}
    The \dfn{identity transformation} on $V$, denoted by $\id_V$, is a transformation that maps each element of $V$ to itself.

In other words,
$$\id_V:V\rightarrow V$$ is a transformation such that $$\id_V(\vec{v})=\vec{v}\quad\text{for all}\quad \vec{v} \in V$$
\end{expandable}

image of a linear transformation (\ref{def:imageofT})
\begin{expandable}{}{}
    Let $V$ and $W$ be vector spaces, and let $T:V\rightarrow W$ be a linear transformation.  The \dfn{image} of $T$, denoted by $\mbox{im}(T)$, is the set
$$\mbox{im}(T)=\{T(\vec{v}):\vec{v}\in V\}$$
In other words, the image of $T$ consists of individual images of all vectors of $V$.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0010/main}{inconsistent system}
\begin{expandable}{}{}
    A system of equations that has no solution.
\end{expandable}

inner product (\ref{def:innerproductspace})
\begin{expandable}{}{}
    An \dfn{inner product} on a real vector space $V$ is a function that assigns a real number $\langle\vec{v}, \vec{w}\rangle$ to every pair $\vec{v}$, $\vec{w}$ of vectors in $V$ in such a way that the following properties are satisfied.

\begin{enumerate}
\item  $\langle\vec{v}, \vec{w}\rangle$ \textit{is a real number for all} $\vec{v}$ \textit{and} $\vec{w}$ \textit{in} $V$.

\item  $\langle\vec{v}, \vec{w}\rangle = \langle\vec{w}, \vec{v}\rangle$ \textit{for all} $\vec{v}$ \textit{and} $\vec{w}$ \textit{in} $V$.

\item  $\langle\vec{v} + \vec{w}, \vec{u}\rangle = \langle\vec{v}, \vec{u}\rangle + \langle\vec{w}, \vec{u}\rangle$ \textit{for all} $\vec{u}$, $\vec{v}$, \textit{and} $\vec{w}$ \textit{in} $V$.

\item $\langle r\vec{v}, \vec{w}\rangle = r\langle\vec{v}, \vec{w}\rangle$ \textit{for all} $\vec{v}$ \textit{and} $\vec{w}$ \textit{in} $V$ \textit{and all} $r$ \textit{in} $\RR$.

\item  $\langle\vec{v}, \vec{v}\rangle > 0$ \textit{for all} $\vec{v} \neq \vec{0}$ \textit{in} $V$.

\end{enumerate}
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VSP-0070/main}{inner product space}

inverse of a linear transformation (\ref{def:inverseoflintrans})
\begin{expandable}{}{}
    Let $V$ and $W$ be vector spaces, and let $T:V\rightarrow W$ be a linear transformation.  A transformation $S:W\rightarrow V$ that satisfies $S\circ T=\id_V$ and $T\circ S=\id_W$ is called an \dfn{inverse} of $T$. If $T$ has an inverse, $T$ is called \dfn{invertible}.
\end{expandable}

inverse of a square matrix (\ref{def:matinverse})
\begin{expandable}{}{}
    Let $A$ be an $n\times n$ matrix.  An $n\times n$ matrix $B$ is called an \dfn{inverse} of $A$ if 
$$AB=BA=I$$
where $I$ is an $n\times n$ identity matrix.  If such an inverse matrix exists, we say that $A$ is \dfn{invertible}.  If an inverse does not exist, we say that $A$ is not invertible.  The inverse of $A$ is denoted by $A^{-1}$.
\end{expandable}

isomorphism (\ref{def:isomorphism})
\begin{expandable}{}{}
    Let $V$ and $W$ be vector spaces.  If there exists an invertible linear transformation $T:V\rightarrow W$ we say that $V$ and $W$ are \dfn{isomorphic} and write $V\cong W$.  The invertible linear transformation $T$ is called an \dfn{isomorphism}.
\end{expandable}

%iterate

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0040/main}{iterative method}
\begin{expandable}{}{}
    A technique where we repeat the same procedure (called an \dfn{iteration}) many times (usually using a computer), and we obtain approximate solutions which we hope ``converge to'' the actual solution.
\end{expandable}

\subsection*{J}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0040/main}{Jacobi's method}
\begin{expandable}{}{}
    An iterative method for solving a system of equations where one variable is isolated in each equation in order to compute the coordinate of the next iterate.
\end{expandable}

\subsection*{K}

kernel of a linear transformation (\ref{def:kernel})
\begin{expandable}{}{}
    Let $V$ and $W$ be vector spaces, and let $T:V\rightarrow W$ be a linear transformation.  The \dfn{kernel} of $T$, denoted by $\mbox{ker}(T)$, is the set
$$\mbox{ker}(T)=\{\vec{v}:T(\vec{v})=\vec{0}\}$$
In other words, the kernel of $T$ consists of all vectors of $V$ that map to $\vec{0}$ in $W$.
\end{expandable}

\subsection*{L}

Laplace Expansion Theorem (\ref{th:laplace1})
\begin{expandable}{}{}
    The determinant of a matrix can be computed using cofactor expansion along ANY row or ANY column.
\end{expandable}

leading entry (leading 1) (\ref{def:leadentry})
\begin{expandable}{}{}
    The first non-zero entry in a row of a matrix (when read from left to right) is called the \dfn{leading entry}.  When the leading entry is 1, we refer to it as a \dfn{leading 1}.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0020/main}{leading variable} (also called a \dfn{basic variable}
\begin{expandable}{}{}
    When a coefficient matrix is in row echelon form, a \dfn{leading variable} is a variable corresponding to a column of the matrix with at least one leading entry.
\end{expandable}

% least squares error %wait and see

% least squares solution %wait and see

linear combination of vectors (\ref{def:lincomb})
\begin{expandable}{}{}
    A vector $\vec{v}$ is said to be a \dfn{linear combination} of vectors $\vec{v}_1, \vec{v}_2,\ldots, \vec{v}_n$ if 
$$\vec{v}=a_1\vec{v}_1+ a_2\vec{v}_2+\ldots + a_n\vec{v}_n$$
for some scalars $a_1, a_2, \ldots ,a_n$.
\end{expandable}

%linear dependence/independence of matrices

linear equation (\ref{def:lineq})
\begin{expandable}{}{}
    A \dfn{linear equation} in variables $x_1, \ldots, x_n$ is an equation that can be written in the form
$$a_1x_1+a_2x_2+\ldots +a_nx_n=b$$
where $a_1,\ldots ,a_n$ and $b$ are constants.
\end{expandable}

linear transformation (\ref{def:lin}) (also see \href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/LTR-0022/main}{Linear Transformations of Abstract Vector Spaces})
\begin{expandable}{}{}
    A transformation $T:\RR^n\rightarrow \RR^m$ is called a \dfn{ linear transformation} if the following are true for all vectors $\vec{u}$ and $\vec{v}$ in $\RR^n$, and scalars $k$.
\begin{equation}
T(k\vec{u})= kT(\vec{u})
\end{equation}
\begin{equation}
T(\vec{u}+\vec{v})= T(\vec{u})+T(\vec{v})
\end{equation}
\end{expandable}

linearly dependent vectors (\ref{def:linearindependence})
\begin{expandable}{}{}
    Let $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_k$ be vectors of $\RR^n$.  We say that the set $\{\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_k\}$ is \dfn{linearly independent} if the only solution to 
\begin{equation}c_1\vec{v}_1+c_2\vec{v}_2+\ldots +c_p\vec{v}_k=\vec{0}\end{equation}
is the \dfn{trivial solution} $c_1=c_2=\ldots =c_k=0$.

If, in addition to the trivial solution, a \dfn{non-trivial solution} (not all $c_1, c_2,\ldots ,c_k$ are zero) exists, then we say that the set $\{\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_k\}$ is \dfn{linearly dependent}.
\end{expandable}

linearly independent vectors (\ref{def:linearindependence})
\begin{expandable}{}{}
    Let $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_k$ be vectors of $\RR^n$.  We say that the set $\{\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_k\}$ is \dfn{linearly independent} if the only solution to 
\begin{equation}c_1\vec{v}_1+c_2\vec{v}_2+\ldots +c_p\vec{v}_k=\vec{0}\end{equation}
is the \dfn{trivial solution} $c_1=c_2=\ldots =c_k=0$.

If, in addition to the trivial solution, a \dfn{non-trivial solution} (not all $c_1, c_2,\ldots ,c_k$ are zero) exists, then we say that the set $\{\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_k\}$ is \dfn{linearly dependent}.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0070/main}{lower triangular matrix}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0070/main}{LU factorization}
\begin{expandable}{}{}
    A factorization $A=LU$ where $L$ is lower triangular and $U$ is upper triangular with ones on the diagonal (called unit upper triangluar).  It is useful for solving $Ax=b$.
\end{expandable}

\subsection*{M}
Magnitude of a vector (\ref{def:normrn})
\begin{expandable}{}{}
  %  \begin{definition}
Let $\vec{v}=\begin{bmatrix}v_1\\ v_2\\ \vdots \\v_n\end{bmatrix}$ be a vector in $\RR^n$, then the \dfn{length}, or the \dfn{magnitude}, of $\vec{v}$ is given by
$$  \norm{\vec{v}}=\sqrt{v_1^2+v_2^2+\ldots +v_n^2}$$
%\end{definition}
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0010/main}{main diagonal}
    
\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0010/main}{matrix}
\begin{expandable}{}{}
    A rectangular array of numbers.  It has $m$ rows and $n$ columns for some positive integers $m$ and $n$.
\end{expandable}

matrix addition (\ref{def:additionofmatrices})
\begin{expandable}{}{}
    Let $A=\begin{bmatrix} a_{ij}\end{bmatrix} $ and $B=\begin{bmatrix} b_{ij}\end{bmatrix}$ be two
$m\times n$ matrices. Then the \dfn{sum of matrices} $A$ and $B$, denoted by $A+B$,  is an $m \times n$
matrix  given by 
$$A+B=\begin{bmatrix}a_{ij}+b_{ij}\end{bmatrix}$$
\end{expandable}

matrix equality (\ref{def:equalityofmatrices})
\begin{expandable}{}{}
    Let $A=\begin{bmatrix} a_{ij}\end{bmatrix}$ and $B=\begin{bmatrix} b_{ij}\end{bmatrix}$ be two $m \times n$ matrices. Then $A=B$ means
that $a_{ij}=b_{ij}$ for all $1\leq i\leq m$ and 
$1\leq j\leq n$.
\end{expandable}

matrix factorization (see \href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0070/main}{LU factorization}, eigenvalue decomposition (\ref{def:eigdecomposition}), QR factorization (\ref{def:QR-factorization}), and \href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RTH-0060/main}{singular value decomposition (SVD)}. 
\begin{expandable}{}{}
    Representing a matrix as a product of two or more matrices.
\end{expandable}

matrix multiplication (by a matrix) (\ref{def:matmatproduct})
\begin{expandable}{}{}
    Let $A$ be an $m\times n$ matrix whose rows are vectors $\vec{r}_1$, $\vec{r}_2,\ldots ,\vec{r}_n$.  Let $B$ be an $n\times p$ matrix with columns $\vec{b}_1, \vec{b}_2, \ldots, \vec{b}_p$.  Then the matrix product $AB$ is an $m \times p$ matrix with entries given by the dot products
$$AB=\begin{bmatrix}-&\vec{r}_1&-\\-&\vec{r}_2&-\\ &\vdots & \\-&\vec{r}_i &-\\ &\vdots& \\-&\vec{r}_m&-\end{bmatrix}\begin{bmatrix}|&|&&|&&|\\\vec{b}_1& \vec{b}_2 &\ldots  & \vec{b}_j&\ldots& \vec{b}_p\\|&|&&|&&|\end{bmatrix}=$$
$$=\begin{bmatrix}\vec{r}_1\dotp \vec{b}_1&\vec{r}_1\dotp \vec{b}_2&\ldots&\vec{r}_1\dotp \vec{b}_j&\ldots &\vec{r}_1\dotp \vec{b}_p\\\vec{r}_2\dotp \vec{b}_1&\vec{r}_2\dotp \vec{b}_2&\ldots&\vec{r}_2\dotp \vec{b}_j&\ldots &\vec{r}_2\dotp \vec{b}_p\\\vdots&\vdots&&\vdots&&\vdots\\\vec{r}_i\dotp \vec{b}_1&\vec{r}_i\dotp \vec{b}_2&\ldots&\vec{r}_i\dotp \vec{b}_j&\ldots &\vec{r}_i\dotp \vec{b}_p\\\vdots&\vdots&&\vdots&&\vdots\\\vec{r}_m\dotp \vec{b}_1&\vec{r}_m\dotp \vec{b}_2&\ldots&\vec{r}_m\dotp \vec{b}_j&\ldots &\vec{r}_m\dotp \vec{b}_p\end{bmatrix}
$$
\end{expandable}

matrix multiplication (by a scalar) (\ref{def:scalarmultofmatrices})
\begin{expandable}{}{}
    If $A=\begin{bmatrix} a_{ij}\end{bmatrix} $ and $k$ is a scalar,
then $kA=\begin{bmatrix} ka_{ij}\end{bmatrix}$. 
\end{expandable}

matrix multiplication (by a vector) (\ref{def:matrixvectormult})
\begin{expandable}{}{}
    Let $A$ be an $m\times n$ matrix, and let $\vec{x}$ be an $n\times 1$ vector.  The product $A\vec{x}$ is the $m\times 1$ vector given by:
$$A\vec{x}=\begin{bmatrix}
           a_{11} & a_{12}&\dots&a_{1n}\\
           a_{21}&a_{22} &\dots &a_{2n}\\
		\vdots & \vdots&\ddots &\vdots\\
		a_{m1}&\dots &\dots &a_{mn}
         \end{bmatrix}\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix}=
         x_1\begin{bmatrix}a_{11}\\a_{21}\\ \vdots \\a_{m1}\end{bmatrix}+
         x_2\begin{bmatrix}a_{12}\\a_{22}\\ \vdots \\a_{m2}\end{bmatrix}+\dots+
         x_n\begin{bmatrix}a_{1n}\\a_{2n}\\ \vdots \\a_{mn}\end{bmatrix}$$
or, equivalently,
$$A\vec{x}=\begin{bmatrix}a_{11}x_1+a_{12}x_2+\ldots +a_{1n}x_n\\a_{21}x_1+a_{22}x_2+\ldots +a_{2n}x_n\\\vdots\\a_{m1}x_1+a_{m2}x_2+\ldots +a_{mn}x_n\end{bmatrix}$$
\end{expandable}

matrix of a linear transformation with respect to the given bases (\ref{def:matlintransgenera})
\begin{expandable}{}{}
    Matrix $A$ of Theorem \ref{th:matlintransgeneral} is called the matrix of $T$ with respect to ordered bases $\mathcal{B}$ and $\mathcal{C}$.
\end{expandable}

matrix powers (\ref{exp:motivate_diagonalization})
\begin{expandable}{}{}
    If $A$ is a square matrix then we can define $A^k$ to be the result of multiplying $A$ by itself $k$ times.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/DET-0010/main}{minor of a square matrix}

\subsection*{N}

negative of a matrix
(\ref{th:propertiesofaddition})
\begin{expandable}{}{}
    The additive inverse of a matrix, formed by multiplying the matrix by the scalar $-1$.
\end{expandable}

\begin{tikzpicture}[scale=1]
   \filldraw[teal, opacity=0.3](0,0)--(20,0)--(20,0.1)--(0,0.1)--cycle;
 \end{tikzpicture}

norm (\ref{def:030438})


nonsingular matrix (\ref{def:nonsingularmatrix})
\begin{expandable}{}{}
    A square matrix $A$ is said to be \dfn{nonsingular} provided that $\mbox{rref}(A)=I$.  Otherwise we say that $A$ is \dfn{singular}.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RRN-0030/main}{normal vector}

null space of a matrix (\ref{def:nullspace})
\begin{expandable}{}{}
    Let $A$ be an $m\times n$ matrix.  The \dfn{null space} of $A$, denoted by $\mbox{null}(A)$, is the set of all vectors $\vec{x}$ in $\RR^n$ such that $A\vec{x}=\vec{0}$.   
    It is a subspace of $\RR^n$.
\end{expandable}

nullity of a linear transformation (\ref{def:nullityT})
\begin{expandable}{}{}
    The \dfn{nullity} of a linear transformation $T:V\rightarrow W$, is the dimension of the kernel of $T$.
$$\mbox{nullity}(T)=\mbox{dim}(\mbox{ker}(T))$$
\end{expandable}

nullity of a matrix (\ref{def:matrixnullity})
\begin{expandable}{}{}
    Let $A$ be a matrix.  The dimension of the null space of $A$ is called the \dfn{nullity} of $A$.
$$\mbox{dim}\Big(\mbox{null}(A)\Big)=\mbox{nullity}(A)$$
\end{expandable}


\subsection*{O}
one-to-one (\ref{def:onetoone})
\begin{expandable}{}{}
    A linear transformation $T:V\rightarrow W$ is \dfn{one-to-one} if 
$$T(\vec{v}_1)=T(\vec{v}_2)\quad \text{implies that}\quad \vec{v}_1=\vec{v}_2$$
\end{expandable}

onto (\ref{def:onto})
\begin{expandable}{}{}
    A linear transformation $T:V\rightarrow W$ is \dfn{onto} if for every element $\vec{w}$ of $W$, there exists an element $\vec{v}$ of $V$ such that $T(\vec{v})=\vec{w}$.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VSP-0060/main}{ordered basis}
\begin{expandable}{}{}
    A basis in which the elements appear in a specific fixed order.  Establishing an order is necessary because a coordinate vector with respect to a given basis relies on the order in which the basis elements appear.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RTH-0010/main}{orthogonal basis}
\begin{expandable}{}{}
    A set of orthogonal vectors that spans a subspace. (Any orthogonal set of vectors must be linearly independent by Theorem \ref{orthbasis}.)
\end{expandable}

orthogonal complement of a subspace (\ref{def:023776})
\begin{expandable}{}{}
    If $W$ is a subspace, we define the \dfn{orthogonal complement} $W^\perp$ as the set of all vectors orthogonal to every vector in $W$, i.e.,
\begin{equation*}
W^\perp = \{\vec{x} \in\RR^n \mid \vec{x} \dotp \vec{y} = 0 \mbox{ for all } \vec{y} \in W\}
\end{equation*}
\end{expandable}

Orthogonal Decomposition Theorem (\ref{th:OrthoDecomp})
\begin{expandable}{}{}
    Let $W$ be a subspace of $\RR^n$ and let $\vec{x} \in \RR^n$.  Then there exist unique vectors $\vec{w} \in W$ and $\vec{w}^\perp \in W^\perp$ such that $\vec{x} = \vec{w} + \vec{w}^\perp$.
\end{expandable}

orthogonal matrix (\ref{def:orthogonal matrices})
\begin{expandable}{}{}
    An $n \times n$ matrix $Q$ is called an \dfn{orthogonal matrix} if its columns form an orthonormal set.  This will happen if and only if its rows form an orthonormal set.  Note also that $Q$ is an orthogonal matrix if and only if it is an invertible matrix such that $Q^{-1}=Q^{T}$.
\end{expandable}

Orthogonal projection onto a subspace of $\RR^n$ (\ref{def:projOntoSubspace})
\begin{expandable}{}{}
    Let $W$ be a subspace of $\RR^n$ with orthogonal basis $\{\vec{f}_{1}, \vec{f}_{2}, \dots, \vec{f}_{m}\}$. If $\vec{x}$ is in $\RR^n$, the vector
\begin{equation}
\vec{w}=\mbox{proj}_W(\vec{x}) = \mbox{proj}_{\vec{f}_1}\vec{x} + \mbox{proj}_{\vec{f}_2}\vec{x} + \dots + \mbox{proj}_{\vec{f}_m}\vec{x}
\end{equation}
is called the \dfn{orthogonal projection} of $\vec{x}$ onto $W$. 
\end{expandable}

Orthogonal projection onto a vector (\ref{def:projection})
\begin{expandable}{}{}
    Let $\vec{v}$ be a vector, and let $\vec{d}$ be a non-zero vector.  The \dfn{projection of $\vec{v}$ onto $\vec{d}$} is given by 
$$\mbox{proj}_{\vec{d}}\vec{v}=\left(\frac{\vec{v}\dotp\vec{d}}{\norm{\vec{d}}^2}\right)\vec{d}$$
\end{expandable}

orthogonal set of vectors (\ref{orthset})
\begin{expandable}{}{}
    Let $\{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k \}$ be a set of nonzero
vectors in $\RR^n$. Then this set is called an
\dfn{orthogonal set} if 
$\vec{v}_i \dotp \vec{v}_j = 0$ for all $i \neq j$.
Moreover, if $\norm{\vec{v}_i}=1$ for $i=1,\ldots,m$ (i.e. each vector in the set is a unit vector), we say the set of vectors is an \dfn{orthonormal set}.
\end{expandable}

Orthogonal vectors (\ref{def:orthovectors}) 
\begin{expandable}{}{}
 %   \begin{definition}
Let $\vec{u}$ and $\vec{v}$ be vectors in $\RR^n$. We say $\vec{u}$ and $\vec{v}$ are \dfn{orthogonal} if $\vec{u}\dotp \vec{v}=0$.
%\end{definition}
\end{expandable}


orthogonally diagonalizable matrix (\ref{def:orthDiag})
\begin{expandable}{}{}
    An $n \times n$ matrix $A$ is said to be \dfn{orthogonally diagonalizable} if an orthogonal matrix $Q$ can be found such that  $Q^{-1}AQ = Q^{T}AQ$ is diagonal.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RTH-0010/main}{orthonormal basis}
\begin{expandable}{}{}
    A set of orthonormal vectors that spans a subspace. (Any orthogonal set of vectors must be linearly independent by Theorem \ref{orthbasis}.)
\end{expandable}

orthonormal set of vectors (\ref{orthset})
\begin{expandable}{}{}
    Let $\{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k \}$ be a set of nonzero
vectors in $\RR^n$. Then this set is called an
\dfn{orthogonal set} if 
$\vec{v}_i \dotp \vec{v}_j = 0$ for all $i \neq j$.
Moreover, if $\norm{\vec{v}_i}=1$ for $i=1,\ldots,m$ (i.e. each vector in the set is a unit vector), we say the set of vectors is an \dfn{orthonormal set}.
\end{expandable}

%outer product


\subsection*{P}

parametric equation of a line (\ref{form:paramlinend})
\begin{expandable}{}{}
    Let $\vec{v}=\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}$ be a direction vector for line $l$ in $\RR^n$, and let $(a_1, a_2,\ldots , a_n)$ be an arbitrary point on $l$.  Then the following parametric equations describe $l$:
\[
x_1=v_1t+a_1\]
\[x_2=v_2t+a_2\]
\[\vdots\]
\[x_n=v_nt+a_n
\]
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0050/main}{particular solution}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0023/main}{partitioned matrices (block multiplication)}
\begin{expandable}{}{}
    Subdividing a matrix into submatrices using imaginary horizontal and vertical lines - used to multiply matrices more efficiently.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0070/main}{permutation matrix}
\begin{expandable}{}{}
    A matrix formed by permuting the rows of the identity matrix.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0030/main}{pivot}
\begin{expandable}{}{}
    In Gaussian elimination, an entry chosen to become a leading coefficient used to get zeros in the remaining rows.
\end{expandable}

positive definite matrix (\ref{def:024811})
\begin{expandable}{}{}
    A square matrix is called \dfn{positive definite} if it is symmetric and all its eigenvalues $\lambda$ are positive.  We write $\lambda>0$ when eigenvalues are real and positive.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/EIG-0070/main}{power method (and its variants)}
\begin{expandable}{}{}
    The \dfn{power method} is an iterative method for computing the dominant eigenvalue of a matrix. It variants can compute the smallest eigenvalue or the eigenvalue closest to some target.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/DET-0040/main}{properties of determinants)}
\begin{expandable}{}{}
\begin{enumerate}
    \item The determinant of a triangular matrix is the product of its diagonal entries.

    \item The determinant of a matrix is equal to the determinant of its transpose.

    \item The determinant of the inverse of a matrix is the reciprocal of the determinant of the matrix.

    \item A matrix with a zero row has determinant zero.

    \item Interchanging two rows of a matrix changes the sign of its determinant.

    \item A matrix with two identical rows has determinant zero.

    \item Multiplying a row of a matrix by $k$ changes the determinant by a factor of $k$.

    \item Multiplying a matrix by $k$ changes the determinant by a factor of $k^n$.

    \item Adding a multiple of one row of a matrix to another row does not change the determinant. 

    \item A matrix is singular if and only if its determinant is zero.

    \item The determinant of a product is equal to the product of the determinants.
\end{enumerate}
\end{expandable}

% properties of matrix algebra 

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RTH-0035/main}{properties of orthogonal matrices)}
\begin{expandable}{}{}
If $Q$ is an orthogonal matrix, then...
\begin{enumerate}
    \item $Q^{-1} = Q^T$ is orthogonal,

    \item $\mbox{det} Q = \pm 1$,

    \item if $\lambda$ is an eigenvalue of $Q$, then $|\lambda|=1$,

    \item the product of $Q$ with any other orthogonal matrix will be an orthogonal matrix (i.e. orthogonal matrices are closed under matrix multiplication), 

    and

    \item $Q$ is a length-preserving and angle-preserving linear transformation.

\end{enumerate}
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/EIG-0040/main}{properties of similar matrices)}
\begin{expandable}{}{}
Similar matrices must have the same...
\begin{enumerate}
    \item determinant,

    \item rank,

    \item trace,

    \item characteristic polynomial, 

    and

    \item eigenvalues.

\end{enumerate}
\end{expandable}

\subsection*{Q}

QR factorization (\ref{def:QR-factorization})
\begin{expandable}{}{}
    Let $A$ be an $m \times n$ matrix with independent columns. A \dfn{QR-factorization} of $A$ expresses it as $A = QR$ where $Q$ is $m \times n$ with orthonormal columns and $R$ is an invertible and upper triangular matrix with positive diagonal entries.
\end{expandable}

\subsection*{R}

%range of a linear transformation

rank of a linear transformation (\ref{def:rankofT})
\begin{expandable}{}{}
    The \dfn{rank} of a linear transformation $T:V\rightarrow W$, is the dimension of the image of $T$.
$$\mbox{rank}(T)=\mbox{dim}(\mbox{im}(T))$$
\end{expandable}

rank of a matrix (\ref{def:rankofamatrix}) (\ref{th:dimofrowA})
\begin{expandable}{}{}
    The \dfn{rank} of matrix $A$, denoted by $\mbox{rank}(A)$, is the number of nonzero rows that remain after we reduce $A$ to row-echelon form by elementary row operations.

    For any matrix $A$,  
$$\mbox{rank}(A)=\mbox{dim}\Big(\mbox{row}(A)\Big)$$

\end{expandable}

Rank-Nullity Theorem for linear transformations (\ref{th:ranknullityforT})
\begin{expandable}{}{}
    Let $T:V\rightarrow W$ be a linear transformation.  Suppose $\mbox{dim}(V)=n$, then
$$\mbox{rank}(T)+\mbox{nullity}(T)=n$$
\end{expandable}

Rank-Nullity Theorem for matrices (\ref{th:matrixranknullity})
\begin{expandable}{}{}
    Let $A$ be an $m\times n$ matrix.  Then 
$$\mbox{rank}(A)+\mbox{nullity}(A)=n$$
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/EIG-0070/main}{Rayleigh quotients}

reduced row echelon form (\ref{def:rref})
\begin{expandable}{}{}
    A matrix that is already in \dfn{row-echelon} form is said to be in \dfn{reduced row-echelon form} if:
\begin{enumerate}
\item Each leading entry is $1$
\item All entries {\it above} and below each leading $1$ are $0$
\end{enumerate}
\end{expandable}

redundant vectors (\ref{def:redundant})
\begin{expandable}{}{}
    Let $\{\vec{v}_1,\vec{v}_2,\dots,\vec{v}_k\}$ be a set of vectors in $\RR^n$.  If we can remove one vector without changing the span of this set, then that vector is \dfn{redundant}.  In other words, if $$\mbox{span}\left(\vec{v}_1,\vec{v}_2,\dots,\vec{v}_k\right)=\mbox{span}\left(\vec{v}_1,\vec{v}_2,\dots,\vec{v}_{j-1},\vec{v}_{j+1},\dots,\vec{v}_k\right)$$ we say that $\vec{v}_j$ is a redundant element of $\{\vec{v}_1,\vec{v}_2,\dots,\vec{v}_k\}$, or simply redundant.
\end{expandable}

%representation of matrix products

row echelon form (\ref{def:ref})
\begin{expandable}{}{}
    A matrix is said to be in \dfn{row-echelon form} if:
\begin{enumerate}
\item All entries below each leading entry are 0.
\item Each leading entry is in a column to the right of the leading entries in the rows above it.
\item All rows of zeros, if there are any, are located below non-zero rows.
\end{enumerate}
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0020/main}{row equivalent matrices}
\begin{expandable}{}{}
    Two matrices $A$ and $B$ are said to be row equivalent if there is a sequence of elementary row operations that converts $A$ to $B$.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0010/main}{row matrix (vector)}
\begin{expandable}{}{}
    A matrix with only $1$ row and $n$ columns.
\end{expandable}

row space of a matrix (\ref{def:rowspace})
\begin{expandable}{}{}
    Let $A$ be an $m\times n$ matrix.  The \dfn{row space} of $A$, denoted by $\mbox{row}(A)$, is the subspace of $\RR^n$ spanned by the rows of $A$.
\end{expandable}


\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0010/main}{row matrix}
\begin{expandable}{}{}
    A matrix with only $1$ row and $n$ columns.
\end{expandable}


\subsection*{S}
\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VEC-0010/main}{Scalar}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/DET-0070/main}{scalar triple product}

scalar multiple of a matrix
(\ref{def:scalarmultofmatrices})
\begin{expandable}{}{}
    If $A=\begin{bmatrix} a_{ij}\end{bmatrix} $ and $k$ is a scalar,
then $kA=\begin{bmatrix} ka_{ij}\end{bmatrix}$. 
\end{expandable}

similar matrices (\ref{def:similar})
\begin{expandable}{}{}
    If $A$ and $B$ are $n \times n$ matrices, we say that $A$ and $B$ are \dfn{similar}, if $B = P^{-1}AP$ for some invertible matrix $P$.  In this case we write $A \sim B$.
\end{expandable}

singular matrix (\ref{def:nonsingularmatrix})
\begin{expandable}{}{}
    A square matrix $A$ is said to be \dfn{singular} provided that $\mbox{rref}(A)$ is NOT the identity matrix.  If instead $\mbox{rref}(A)=I$, we say that $A$ is \dfn{nonsingular}.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RTH-0060/main}{singular value decomposition (SVD)}

singular values (\ref{singularvalues})
\begin{expandable}{}{}
    Let $A$ be an $m\times n$ matrix. The \dfn{singular values} of $A$ are the square roots of the positive
eigenvalues of $A^TA.$ 
\end{expandable}

skew symmetric matrix (\ref{def:symmetricandskewsymmetric})
\begin{expandable}{}{}
    An $n\times n$ matrix $A$ is said to be
\dfn{symmetric} if $A=A^{T}.$ It is said to be
\dfn{skew symmetric} if $A=-A^{T}.$
\end{expandable}

%span of a set of matrices

span of a set of vectors (\ref{def:span})
\begin{expandable}{}{}
    Let $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$ be vectors in $\RR^n$.  The set $S$ of all linear combinations of $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$ is called the \dfn{span} of $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$.  We write 
$$S=\mbox{span}(\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p)$$
and we say that vectors $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$ \dfn{span} $S$.  Any vector in $S$ is said to be \dfn{in the span} of $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$.  The set $\{\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p\}$ is called a \dfn{spanning set} for $S$.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VEC-0090/main}{spanning set}
\begin{expandable}{}{}
    Let $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$ be vectors in $\RR^n$.  The set $S$ of all linear combinations of $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$ is called the \dfn{span} of $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$.  We write 
$$S=\mbox{span}(\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p)$$
and we say that vectors $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$ \dfn{span} $S$.  Any vector in $S$ is said to be \dfn{in the span} of $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$.  The set $\{\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p\}$ is called a \dfn{spanning set} for $S$.
\end{expandable}

spectral decomposition - another name for eigenvalue decomposition (\ref{def:eigdecomposition})
\begin{expandable}{}{}
    If we are able to diagonalize $A$, say $A=PDP^{-1}$, we say that $PDP^{-1}$ is an \dfn{eigenvalue decomposition} of $A$.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RTH-0035/main}{Spectral Theorem}
\begin{expandable}{}{}
    If $A$ is a real $n \times n$ matrix, then $A$ is symmetric if an only if $A$ is orthogonally diagonalizable.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/RTH-0035/main}{spectrum} 
The set of distinct eigenvalues of a matrix.


\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0010/main}
{square matrix}
\begin{expandable}{}{}
    A matrix with the same number of rows and columns.
\end{expandable}

standard basis (\ref{def:standardbasis})
\begin{expandable}{}{}
    The set $\{\vec{e}_1, \ldots ,\vec{e}_n\}$ is called the \dfn{standard basis} of $\RR^n$.
\end{expandable}

standard matrix of a linear transformation (\ref{def:standardmatoflintrans})
\begin{expandable}{}{}
    Let $T:\RR^n\rightarrow\RR^m$ be a linear transformation.  Then the matrix
  \begin{equation*} \label{matlintrans}
 A=\begin{bmatrix}
           | & |& &|\\
		T(\vec{e}_1) & T(\vec{e}_2)&\dots &T(\vec{e}_n)\\
		|&| & &|
         \end{bmatrix}
\end{equation*}
is known as the \dfn{standard matrix of the linear transformation} $T$.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VEC-0010/main}{Standard Position}

Standard Unit Vectors (\ref{def:standardunitvecrn})
\begin{expandable}{}{}
    %\begin{definition}
  Let $\vec{e}_i$ denote a vector that has $1$ as the $i^{th}$ component and zeros elsewhere.  In other words, $$\vec{e}_i=\begin{bmatrix}
0\\
0\\
\vdots\\
1\\
\vdots\\
0
\end{bmatrix}$$ 
  where $1$ is in the $i^{th}$ position.  We say that  $\vec{e}_i$ is a \dfn{standard unit vector of $\RR^n$}.
%\end{definition}
\end{expandable}

strictly diagonally dominant (\ref{def:strict_diag_dom})
\begin{expandable}{}{}
    Let $A=[a_{ij}]$ be the $n\times n$ matrix which is the coefficient matrix of the linear system $A \vec{x}= \vec{b}$.  Let
$$
r_i(A):= \sum_{j \ne i} |a_{ij}|
$$
denote the sum of the absolute values of the non-diagonal entries in row $i$.  We say that $A$ is \dfn{strictly diagonally dominant} if 
$$|a_{ii}|>r_i(A)$$
for all values of $i$ from $i=1$ to $i=n$.
\end{expandable}


subspace (\ref{def:subspaceabstract})
\begin{expandable}{}{}
    A nonempty subset $U$ of a vector space $V$ is called a \dfn{subspace} of $V$, provided that $U$ is itself a vector space when given the same addition and scalar multiplication as $V$.
\end{expandable}

subspace of $\RR^n$ (\ref{def:subspace})
\begin{expandable}{}{}
    Suppose that $V$ is a nonempty subset of $\RR^n$ that is closed under addition and closed under scalar multiplication.  Then $V$ is a \dfn{subspace} of $\RR^n$.
\end{expandable}

subspace test (\ref{th:subspacetestabstract})
\begin{expandable}{}{}
    Let $U$ be a nonempty subset of a vector space $V$.  If $U$ is closed under the operations of addition and scalar multiplication of $V$, then $U$ is a subspace of $V$.
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VEC-0030/main}{Subtraction of vectors}

symmetric matrix (\ref{def:symmetricandskewsymmetric})
\begin{expandable}{}{}
    An $n\times n$ matrix $A$ is said to be
\dfn{symmetric} if $A=A^{T}.$ It is said to be
\dfn{skew symmetric} if $A=-A^{T}.$
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/SYS-0010/main}{system of linear equations}
\begin{expandable}{}{}
    A finite set of linear (degree 1) equations each with the same variables.
\end{expandable}


\subsection*{T}

trace of a matrix (\ref{def:trace})
\begin{expandable}{}{}
    The \dfn{trace} of an $n \times n$ matrix $A$, abbreviated $\mbox{tr} A$, is defined to be the sum of the main diagonal elements of $A$.  In other words, if $ A = [a_{ij}]$, then $$\mbox{tr}(A) = a_{11} + a_{22} + \dots + a_{nn}$$  We may also write $\mbox{tr}(A) =\sum_{i=1}^n a_{ii}$.
\end{expandable}

transpose of a matrix (\ref{def:matrixtranspose})
\begin{expandable}{}{}
    Let $A=\begin{bmatrix} a _{ij}\end{bmatrix}$ be an $m\times n$ matrix. Then the \dfn{transpose of $A$}, denoted by $A^{T}$, is the $n\times m$
matrix given by switching the rows and columns:
\begin{equation*}
A^{T} = \begin{bmatrix} a _{ij}\end{bmatrix}^{T}= \begin{bmatrix} a_{ji} \end{bmatrix}
\end{equation*}
\end{expandable}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/APX-0010/main}{triangle inequality}

\subsection*{U}
\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VEC-0036/main}{Unit Vector}

\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/MAT-0070/main}{upper triangular matrix}

\subsection*{V}
\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VEC-0010/main}{Vector}

Vector equation of a line (\ref{form:vectorlinend})
\begin{expandable}{}{}
    Let $\vec{v}=\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}$ be a direction vector for line $l$ in $\RR^n$, and let $(a_1, a_2,\ldots , a_n)$ be an arbitrary point on $l$.  Then the following vector equation describes $l$:
$$\vec{x}(t)=\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}t+\begin{bmatrix}a_1\\a_2\\\vdots\\a_n\end{bmatrix}$$
\end{expandable}

vector space (\href{https://ximera.osu.edu/linearalgebradzv3/LinearAlgebraInteractiveIntro/VSP-0020/main}{$\RR^n$})
(\ref{def:vectorspacegeneral})
\begin{expandable}{}{}
    Let $V$ be a nonempty set.  Suppose that elements of $V$ can be added together and multiplied by scalars.  The set $V$, together with operations of addition and scalar multiplication, is called a \dfn{vector space} provided that 
  \begin{itemize}
  \item[] $V$ is closed under addition
  \item[] $V$ is closed under scalar multiplication
  \end{itemize}
  and the following properties hold for $\vec{u}$, $\vec{v}$ and $\vec{w}$ in $V$ and scalars $k$ and $p$:
  \begin{enumerate}
   \item 
  Commutative Property of Addition:\quad
  $\vec{u}+\vec{v}=\vec{v}+\vec{u}$
  \item 
  Associative Property of Addition:\quad
  $(\vec{u}+\vec{v})+\vec{w}=\vec{u}+(\vec{v}+\vec{w})$
  \item 
  Existence of Additive Identity:\quad
  $\vec{u}+\vec{0}=\vec{u}$
  \item 
  Existence of Additive Inverse:\quad
  $\vec{u}+(-\vec{u})=\vec{0}$
  \item 
  Distributive Property over Vector Addition:\quad
  $k(\vec{u}+\vec{v})=k\vec{u}+k\vec{v}$
  \item 
  Distributive Property over Scalar Addition:\quad
  $(k+p)\vec{u}=k\vec{u}+p\vec{u}$
  \item 
  Associative Property for Scalar Multiplication:\quad
  $k(p\vec{u})=(kp)\vec{u}$
  \item 
  Multiplication by $1$:\quad
  $1\vec{u}=\vec{u}$
  \end{enumerate}
We will refer to elements of $V$ as \dfn{vectors}.  
\end{expandable}

\subsection*{W}

\subsection*{X}

\subsection*{Y}

\subsection*{Z}

zero matrix (\ref{def:zeromatrix})
\begin{expandable}{}{}
    The $m\times n$ \dfn{zero matrix} is the $m\times n$ matrix
having every entry equal to zero. The zero matrix is
denoted by $O$.
\end{expandable}

zero transformation (\ref{def:zerotransonrn})
\begin{expandable}{}{}
    The \dfn{zero transformation}, $Z$, maps every element of the domain to the zero vector.

In other words,
$$Z:V\rightarrow W$$ is a transformation such that $$Z(\vec{v})=\vec{0}\quad\text{for all}\quad \vec{v} \in V$$
\end{expandable}

\end{document}
